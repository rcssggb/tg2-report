\chapter{Introdução}
\label{chap:Intro}

% Resumo opcional. Comentar se não usar.
% \resumodocapitulo{Resumo opcional}

% TODO: revisar o parágrafo abaixo (feito por pedido do prof. Romariz)
O treinamento de agentes, sejam eles máquinas físicas ou simuladas, para realizar certas tarefas é um campo de estudo crescente em meio à academia e ao setor privado. Em um ambiente de robôs jogadores de futebol simulados, este trabalho tem como alvo estruturar o procedimento de treinamento desses jogadores autônomos.

\par O interesse humano em criar artefatos para facilitar seu próprio trabalho ou realizar uma tarefa sem interferência remonta os tempos mais antigos. No Egito Ptolomaico, Ctesíbio (285-222 AC) descreveu um relógio d`água com a presença de um sistema de engrenagens, um indicador e o primeiro sistema de retroalimentação registrado. Por volta de 1495, Leonardo da Vinci, concebeu o projeto de um autômato mecânico de um guerreiro em armadura medieval que podia ficar em pé, sentar-se, levantar o visor e mover os braços \cite{guarnieri2010}. 
\par O estudo da união de sistemas eletromecânicos e inteligência teve início há pelo menos 70 anos. A \textit{cibernética}, área inaugurada por Norbert Wiener na década de 1950, descreve o estudo científico de controle e comunicação no animal e na máquina. Wiener começou, então, a desenvolver sistemas que replicassem comportamentos animais \cite{wiener1948cybernetics}. Somado a isso, a teoria da informação de Claude Shannon e a teoria de computação de Alan Turing abriram espaço para pesquisas que iriam desenvolver Inteligências Artificiais (IA) \cite{pamela2004}.

\section{Robótica}
\par A robótica se apoia em conhecimentos de vários campos para criar uma das áreas de estudos mais amplas da ciência. Desde a metade do século XX, a robótica vem reunindo noções dessas áreas e, pouco a pouco, tornando-as partes essenciais de si: sistemas mecânicos, eletromecânicos, teoria de controle, IA e outras. As aplicações existentes são inúmeras e se renovam a todo momento. Dentre as principais, é possível citar sistemas de manufatura, robótica médica e robótica agricultural \cite{handbook2007}. 
\par Sistemas robóticos construídos com o objetivo de automatizar tarefas repetitivas se mostram úteis, entretanto o avanço das indústrias e aumento da complexidade das tarefas a serem realizadas criou um ambiente catalisador para o desenvolvimento de processos capazes também de tomar de decisões autonomamente no chão de fábrica.
\par É importante notar a complexidade do problema de se desenvolver a tomada de decisão de um sistema autônomo. Tal sistema precisa mapear seu ambiente por meio de sensores, extrair um significado do seu estado atual, usá-lo para decidir uma ação e determinar se tal ação foi a melhor a ser tomada. Sensores, porém, são imprecisos e limitados fisicamente. A representação dos estados, frequentemente, não é completamente conhecida e o processo de mapeamento de estado para ação não é trivial.
\par Neste contexto, surgiu a área de estudo conhecida como \textit{aprendizagem por reforço}, que formaliza os elementos citados anteriormente para prover uma base de como agentes devem tomar ações para cumprir um objetivo pré-definido \cite{sutton2018reinforcement}.


\section{Aprendizagem por Reforço}
\par A aprendizagem por reforço (do inglês, \textit{reinforcement learning} ou RL) tem como inspiração a maneira como o aprendizado acontece com seres-humanos: interagindo com o ambiente \cite{sutton2018reinforcement}. Se uma criança está aprendendo a andar, por exemplo, ela toma certas ações no ambiente e, ainda que inconscientemente, está atenta aos resultados que essa ação causa. 
\par A teoria por trás da aprendizagem por reforço formaliza a ideia de aprender através da interação e a aplica em um contexto computacional. Os principais elementos de um sistema de RL são: uma política de decisão, um sinal de recompensa e uma função valor.  O primeiro diz respeito à decisão de qual ação se tomar a partir de uma situação, o segundo quantifica quão boa foi a ação escolhida naquele momento e o terceiro quantifica quão boa é a ação considerando o longo prazo \cite{sutton2018reinforcement}.
\par Apesar de recente, a técnica de RL já se mostrou promissora em diversas áreas, com destaque para seu uso em jogos. Em 2016, o programa AlphaGo mostrou resultados significativos ao jogar contra o campeão europeu de Go, superando-o nos 5 jogos disputados \cite{SilverHuangEtAl16nature}.

Em 2018, pesquisadores do grupo OpenAI utilizaram técnicas de RL para treinar um time de 5 agentes colaborativos no jogo \textit{DotA 2}, um jogo de estratégia em tempo real onde 2 times batalham para destruir a base inimiga. O jogo provê um ambiente extremamente complexo, com espaços de estados e ações contínuos. São $20.000$ valores no espectro contínuo (ponto-flutuante), um para cada dimensão do estado, e $1.000$ ações possíveis em um dado ciclo.
% Em contrapartida os estados em um jogo de Go são codificados com 400 números e as ações com aproximadamente 250 números.
% retirei essa parte porque achei que tornou confuso se a informação seguinte era sobre Go ou DotA 2
A duração usual de uma partida é de pelo menos 1 hora. Inicialmente, foi feito um sistema do tipo um contra um (1v1) e o agente resultante deste treinamento foi capaz de derrotar jogadores profissionais. Em 2019, o sistema com 5 agentes foi capaz de derrotar um time profissional \cite{OpenAI_dota}. Um resultado dessa magnitude foi possível devido, entre outras razões, ao número altísssimo de amostras coletadas pelos agentes: 300 anos de experiência por dia para o agente singular e 180 anos por dia por agente para o time contendo 5 membros.

\section{Futebol de Robôs}
\par A ideia de robôs jogando futebol foi proposta pela primeira vez em 1992 por Alan Mackworth \cite{mackworth1993seeing}. Desde então a comunidade científica tem criado iniciativas buscando por soluções que tornem isso realidade.
\par Uma delas é a \textit{Robot World Cup Initiative} \cite{robocup-initiative}, abreviada como \textit{RoboCup}, que teve sua primeira edição em 1997 com mais de 40 equipes distribuídas entre as diversas categorias do evento.
\par O objetivo da iniciativa, definido pela \textit{RoboCup Federation}, é que por volta da metade do século XXI, um time de robôs humanóides autônomos vençam uma partida contra os campeões da Copa do Mundo mais recente. Mesmo que o objetivo pareça ambicioso, ele guia as pesquisas e motiva os avanços no campo.
\par Atualmente, a RoboCup conta com mais de 10 categorias, incluindo robôs humanoides, robôs com rodas e simulações. Entre elas há a \textit{RoboCup Soccer Simulation 2D}, abreviada RCSS, objeto de estudo deste projeto.

\subsection{RoboCup Soccer Simulation 2D}
\par A RCSS possui grande relevância internacional, sendo uma das principais categorias disputadas na RoboCup, com equipes do mundo inteiro.
\par A categoria apresenta, também, grande relevância no cenário brasileiro.
Desde 2005, a RCSS está presente na maior competição de robótica da América Latina, a \textit{Latin American Robotics Competition}, LARC.
\par Nessa categoria, duas equipes de 11 jogadores autônomos e independentes jogam futebol em um ambiente virtual bidimensional. Um servidor é responsável por esse ambiente e possui informação absoluta sobre o estado do jogo e suas regras. Os jogadores, por sua vez, recebem dele informação incompleta e ruidosa de seus sensores virtuais, podendo executar comandos a fim de atuar sobre o estado do jogo \cite{rcssmanual2003}.

\subsection{Servidor da partida}
\label{subsec:server}
\par Um servidor que executa a partida é disponibilizado pelos organizadores da competição e este pode ser utilizado, também, para desenvolvimento. O servidor, portanto, apresenta, internamente, algumas das regras da partida bem como um juiz autônomo que age para determinar gols, faltas e demais situações de uma partida de futebol. Caso necessário, um juiz humano poderá intervir em situações não contempladas pelas regras do servidor.
\par O servidor simula todos os movimentos e ações dos jogadores e da bola. Clientes externos se conectam ao servidor e cada cliente controla um único jogador. Uma partida usual tem 6.000 ciclos de 100 milisegundos, totalizando 10 minutos. A comunição entre o cliente e o servidor é feita a partir do protocolo UDP por meio de mensagens com sintaxe específica e definida pelo servidor.
\par De forma a permitir o acompanhamento visual da partida, um monitor também é disponibilizado, porém não é necessário para que uma partida ocorra com sucesso.
\par O servidor possui, ainda, o modo \textit{trainer} para utilização durante treinamentos de algoritmos de inteligência computacional. Este modo permite a conexão de um cliente do tipo treinador que tem acesso absoluto às informações da partida e pode mudar modos de jogo e ainda mover arbitrariamente jogadores e bola. Adicionalmente, é possível acelerar os ciclos da partida simulando o mais rápido possível e permitindo o treinamento em tempo hábil.

\begin{figure}[h]
	\includegraphics[width=0.9\linewidth]{figs/server.png}
	\centering
	\caption{Visualização de uma partida em andamento.}
	\label{fig:rcssserver}
\end{figure}

\subsection{Cliente}
\par Os jogadores são controlados por clientes externos conectados ao servidor. Como citado, um cliente corresponde a um único jogador e os clientes só podem se comunicar entre si por meio de mensagens enviadas através do servidor da partida.
\par O cliente pode ser desenvolvido em qualquer linguagem desde que se comunique com o servidor via protocolo UDP e utilize a sintaxe de mensagens reconhecida pelo sistema. Há várias escolhas disponíveis para a construção do cliente, sendo decisão de cada equipe competidora como fazê-lo.

\begin{figure}[H]
	\includegraphics[width=0.9\linewidth]{figs/system.png}
	\centering
	\caption{Esquema ilustrando a arquitetura de um cliente e sua comunicação com o servidor do jogo.}
	\label{fig:system}
\end{figure}

\subsection{Sensores e Ações}
\label{sec:actions}

Cada jogador presente na partida possui um conjunto de sensores de onde são tiradas todas as informações sobre o ambiente. Em uma partida usual, um jogador tem informações visuais dos jogadores do seu time e do time adversário, da bola e de uma série de marcadores fixos no campo, como bandeiras e linhas, que servem para situar o jogador em coordenadas absolutas do campo. O jogador possui também informações ``sonoras'', onde pode ouvir mensagem do árbitro, treinador e de outros jogadores. Por último, têm acesso a informações do próprio corpo, como orientação do corpo e pescoço \cite{rcssmanual2003}. Os sensores possuem características que os aproximam de sensores reais como perda de resolução da informação conforme a variável medida se afasta do sensor.

Adicionalmente, a cada ciclo de simulação, cada cliente conectado ao servidor pode realizar ações que terão efeito no ambiente \cite{rcssmanual2003}. As ações englobam mover-se, virar-se, chutar a bola e até falar, permitindo troca de mensagens entre os jogadores. As ações disponíveis serão detalhadas no decorrer do texto.


\section{Trabalhos Relacionados}
\label{subsec:abordagens}

Uma pesquisa sobre as abordagens para o desenvolvimento das estratégias dos times participantes da RCSS revelou o uso recorrente de métodos de inteligência computacional.

A equipe chinesa \textit{WrightEagle}, campeã do principal evento internacional da categoria diversas vezes, utiliza Processos de Decisão de Markov ou MDPs para modelar a partida \cite{bai2015online}.

A equipe japonesa \textit{HELIOS}, campeã de 2018 da categoria na RoboCup, divide seus jogadores em categorias ``chutadores'' e ``não-chutadores''.
Os chutadores são responsáveis por realizar o planejamento de sequência de ações, utilizando métodos de valor de ação.
Os não-chutadores, por sua vez, não têm conhecimento do planejamento feito pelos chutadores e devem obter o máximo de informações relevantes para tentar gerar a mesma sequência de ações que jogador chutador \cite{nakashima2018helios2018}.

A equipe brasileira \textit{ITAndroids}, atual campeã da LARC, utiliza a abordagem de sequência de ações, similar à \textit{HELIOS}, explorando uma árvore de ações criada dinamicamente de forma a maximizar o valor de cada ação. Além disso, utilizam Otimização por Enxame de Partículas \cite{melloitandroids} para adequar os parâmetros que calculam o valor da ação. A \textit{ITAndroids} também vem desenvolvendo o uso de Aprendizagem por Reforço Profunda \cite{maximoitandroids}.

Muitas equipes, ainda, desenvolvem seus agentes utilizando o agente base da equipe \textit{HELIOS}, \textit{Agent2d} com a biblioteca \textit{Librcsc}, escritas em C++. O \textit{Agent2d} utiliza uma abordagem que é chamada de \textit{layered learning}, em que são definidas camadas onde são decididos comportamentos para cada agente dentro de estratégias pré-definidas \cite{nakashima2018helios2018}. Por ser utilizada por várias equipes, é comum que haja semelhança na construção entre agentes. Tal semelhança torna mais fácil a transferência de conhecimento entre as equipes, uma vez que todas utilizam a mesma plataforma. Porém, a reutilização das mesmas soluções acaba por limitar a diversidade de técnicas presentes nos campeonatos, o que pode prejudicar o avanço da área.

A utilização de comportamentos pré-programados, como os utilizados pela plataforma \textit{Agent2d} é muito comum no âmbito do futebol de robôs, bem como em outras áreas onde o ambiente é conhecido e, portanto, simples para um projetista codificar comportamentos a partir de um conjunto de ações elementares. No ambiente de futebol de robôs, são exemplos de comportamentos: seguir bola e chutar bola para o gol.

Entretanto, uma abordagem mais sofisticada seria utilizar as próprias ações elementares como objeto de escolha do agente para haja mais flexibilidade no treinamento, permitindo o agente descobrir comportamentos não pensados pelo projetista. Exemplos de ações elementares seriam: andar, chutar e virar, onde parâmetros ditam como e com qual intensidade a ação será feita.


\section{Caracterização do Problema}

Deseja-se, então, explorar o problema de se fazer gols com um agente único no ambiente descrito utilizando-se de técnicas de aprendizagem por reforço. Para isso, foi desenvolvida uma biblioteca de interfaceamento com o servidor da partida como adaptação do ambiente. Após isso, o agente foi treinado com a utilização de técnicas de RL em duas abordagens - ações elementares e comportamentos - de forma a compará-las.


\subsection{Objetivos Específicos}
\par De acordo com o contexto apresentado, o presente trabalho se propõe a cumprir as seguintes etapas:
\begin{itemize}
	\item Implementar uma plataforma para desenvolvimento de aprendizagem por reforço no âmbito da categoria RCSS;
	\item Utilizar técnicas de aprendizagem por reforço para treinar a escolha de comportamentos;
	\item Utilizar técnicas de aprendizagem por reforço para treinar a escolha de ações elementares;
	\item Comparar as diferentes configurações de treinamento e seus resultados.
\end{itemize}

\section{Estrutura do Trabalho}
\par Este documento está organizado em cinco capítulos: Introdução, Fundamentação Teórica, Desenvolvimento, Resultados e Conclusões. No Capítulo 2, descrevem-se os fundamentos teóricos da aprendizagem por reforço, com destaque para os algoritmos de \textit{Sarsa}, \textit{Q-learning} e \textit{Q-learning} duplo. No Capítulo 3, descrevem-se o desenvolvimento da biblioteca e a definição de ações e comportamentos. Além disso, propõem-se o algoritmo de treinamento e os experimentos a serem realizados. No Capítulo 4, os resultados desses experimentos e suas análises são apresentados. Finalmente, o Capítulo 5 expõe conclusões e trabalhos futuros.

